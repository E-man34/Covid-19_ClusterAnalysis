{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Crowd_mar_apr_TOT_2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2b6af9d45147>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Crowd_mar_apr_TOT_2.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#creo una colonna che contiene tutte le parti di testo analizzabili\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Crowd_mar_apr_TOT_2.csv'"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "df = pd.read_csv('Crowd_mar_apr_TOT_2.csv', sep=';', keep_default_na=False, low_memory=False, error_bad_lines=False)\n",
    "\n",
    "#creo una colonna che contiene tutte le parti di testo analizzabili\n",
    "\n",
    "#df['complessiva']=df['Message'].astype(str)+' ' +df['Image Text'].astype(str)+' '+df['Link Text'].astype(str)+ ' '+df['Description'].astype(str)\n",
    "df['Tot'] = df['Totale'].astype(str)\n",
    "\n",
    "nlp = spacy.load('it_core_news_lg')\n",
    "nlp_en = spacy.load('en_core_web_lg')\n",
    "nlp_es = spacy.load('es_core_news_lg')\n",
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "nlp_de = spacy.load('de_core_news_lg')\n",
    "\n",
    "\n",
    "#metodo per rimuovere siboli e emoticons\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u000D\" #ritorno a capo\n",
    "                               u\"\\u000A\" #newline\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\u0023\" #cancelletto\n",
    "                               u\"\\u0030-\\u0039\"#numeri\n",
    "                               u\"\\u00A0-\\u00BB\" #rimozione virgolette strane\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r' ', string)\n",
    "\n",
    "#metodo non usato al momento\n",
    "def tokenize(texts):\n",
    "    \"\"\"\n",
    "    Tokenizes a list of texts, returning a list of tokens.\n",
    "    \"\"\"\n",
    "    token_texts = []\n",
    "    for text in texts:\n",
    "        token_texts.append(simple_preprocess(text, deacc=True))\n",
    "    return token_texts\n",
    "\n",
    "#metodo per creare i lemmi, contiene una lista di stopword ulteriore\n",
    "def lemming_cell(text):\n",
    "    stopword=['nn','settembrese','x','xke','codvid','di','a','da','in','con','contro','coi','su',\n",
    "              'sui','sullo','sulle','sulla','sugli','sull','per','tra','fra','ho','sono','ad','allo','agli',\n",
    "              'alla','alle','allo','all','col','dal','dei','dallo','dalla','dalle','dagli','dell','il','lo',\n",
    "              'la','i','gli','le','un','uno','una','io','tu','lui','lei','noi','voi','loro','mio','mia',\n",
    "              'miei','mie','tuo','tua','tuoi','tue','suo','sua','suoi','già','gia','eh','cè','ce','poi','boh',\n",
    "              'sue','nostro','nostra','nostri','nostre','vostro','vostra','vostri','vostre','mi','ti','ci',\n",
    "              'vi','ma','ed','se','perché','anche','come','dov','dove','che','chi','cui','non','più','quale',\n",
    "              'quanto','quanti','quanta','quante','quello','quelli','quella','quelle','questo','questi',\n",
    "              'questa','queste','si','no','non','tutto','tutti','a','c','e','i','l','o','ho','hai','ha',\n",
    "              'abbiamo','avete','hanno','abbia','abbiate','abbiano','avere','avrò','avro','avrai','avrà',\n",
    "              'avremo','avrete','avranno','avrei','avresti','avrebbe','avremmo','avreste','avrebbero','avevo',\n",
    "              'avevi','aveva','avevamo','avevate','avevano','ebbi','avesti','ebbe','avemmo','aveste','ebbero',\n",
    "              'avessi','avesse','avessimo','avessero','avendo','avuto','avuta','avuti','avute','essere','sono',\n",
    "              'sei','è','siamo','siete','sia','siate','siano','sarò','saro','sarai','sarà','sara','saremo',\n",
    "              'sarete','saranno','sarei','saresti','sarebbe','saremmo','sareste','sarebbero','ero','eri','era',\n",
    "              'eravamo','eravate','erano','fui','fosti','fu','fummo','foste','furono','fossi','fosse','fossimo',\n",
    "              'fossero','essendo','faccio','fai','facciamo','fanno','faccia','facciate','facciano','farò',\n",
    "              'farai','farà','fara','faremo','farete','faranno','farei','faresti','farebbe','faremmo',\n",
    "              'fareste','farebbero','facevo','facevi','faceva','facevamo','facevate','facevano','feci',\n",
    "              'facesti','fece','facemmo','faceste','fecero','facessi','facesse','facessimo','facessero',\n",
    "              'facendo','sto','stai','sta','stiamo','stanno','stia','stiate','stiano','starò','starai','starà',\n",
    "              'staremo','starete','staranno','starei','staresti','starebbe','staremmo','stareste','starebbero',\n",
    "              'stavo','stavi','stava','stavamo','stavate','stavano','stetti','stesti','stette','stemmo',\n",
    "              'steste','stettero','stessi','stesse','stessimo','stessero','stando','dare','diedi','dai',\n",
    "              'daremo','darete','davo','davi','dava','davamo','davate','davano','dasti','dassero','dando',\n",
    "              'davasti','desti','davammo','davaste','diedero','leggere','letto','leggo','leggiamo','photos',\n",
    "              'leggete','leggono','leggevo','leggeva','leggevi','leggevamo','leggevate','leggevano','leggendo',\n",
    "              'mantenere','manteniamo','mantengo','mantieni','mantiene','mantenete','mantengono','mantenevo',\n",
    "              'mantenevi','manteneva','mantenuto','mantenendo','rispettare','rispetto','rispetti','rispetta',\n",
    "              'rispettiamo','rispettate','rispettano','rispettavo','rispettavi','rispettava','rispettato',\n",
    "              'intervistare','intervistato','intervistavano','andare','andavo','andato','andai','vado','vai',\n",
    "              'va','andiamo','andate','vanno','andando','andava','andavi','arrivare','numerare','mettere',\n",
    "              'messo','metto','sapere','fino','così','cosi','bho']\n",
    "    \n",
    "    stopword_en = nlp_en.Defaults.stop_words\n",
    "    stopword_es = nlp_es.Defaults.stop_words\n",
    "    stopword_fr = nlp_fr.Defaults.stop_words\n",
    "    stopword_de = nlp_de.Defaults.stop_words\n",
    "    \n",
    "    tokens=[] \n",
    "    doc=nlp(text.lower()) #trasforma tutte le parole in minuscolo in minuscolo\n",
    "    \n",
    "    for token in doc:\n",
    "        # se la parola non è un URL e non è nella lista stop word, allora mette il lemma nel vettore \"tokens\"\n",
    "        if token.pos_ in ['PROPN','NOUN','VERB','ADJ','ADV'] and not token.is_stop and not token.like_url and token.lemma_ not in stopword and token.lemma_ not in stopword_en and token.lemma_ not in stopword_es and token.lemma_ not in stopword_fr and token.lemma_ not in stopword_de:\n",
    "            tokens.append(token.lemma_)\n",
    "    tokens=\" \".join(set(tokens)) # trasforma vettore tokens in una stringa\n",
    "    return tokens\n",
    "\"\"\"\n",
    "#costruisco un file con le colonne riunite ma ripulite per TLAB\n",
    "finale=open(\"dati.csv\",\"w\")\n",
    "for i in df['complessiva']:\n",
    "    if len(i)<2000:\n",
    "        pulito=remove_emoji(str(i))\n",
    "        a=pulito.replace('  ',' ')\n",
    "        a=a.replace('\\n','')\n",
    "        finale.write(a)\n",
    "        finale.write('\\n')\n",
    "        \n",
    "finale.close() \n",
    "\"\"\"\n",
    "#costruisco un corpus con i singoli post già puliti da emoticon, da segni di interpunzione e url\n",
    "\n",
    "corpus=[] # corpus è un vettore di stringhe \"pulite\"\n",
    "for i in df['Tot']:\n",
    "    phrase=remove_emoji(str(i))\n",
    "    vett=lemming_cell(phrase)\n",
    "    if vett !=\"\":\n",
    "        corpus.append(vett)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(corpus)\n",
    "dataset.to_excel(\"Dataset_TLAB.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 30 PAROLE DEL CLUSTER #0\n",
      "['parlare', 'dottore', 'sanità', 'mascherina', 'grande', 'materiale', 'lavorare', 'combattere', 'infermiere', 'virus', 'epidemia', 'cina', 'bergamo', 'milano', 'lineare', 'cinese', 'personale', 'lottare', 'aiutare', 'lombardia', 'esperto', 'covid', 'italiano', 'emergenza', 'sanitario', 'ospedale', 'italia', 'medico', 'medicare', 'coronavirus']\n",
      "\n",
      "\n",
      "TOP 30 PAROLE DEL CLUSTER #1\n",
      "['malato', 'lavorare', 'ricoverato', 'guarire', 'intensivo', 'morto', 'ricoverare', 'napoli', 'curare', 'famiglia', 'sindacare', 'caso', 'terapia', 'dottore', 'reparto', 'risultare', 'emergenza', 'notizia', 'personale', 'sanitario', 'morire', 'tampone', 'medico', 'contagiare', 'paziente', 'positivo', 'covid', 'ospedale', 'coronavirus', 'medicare']\n",
      "\n",
      "\n",
      "TOP 30 PAROLE DEL CLUSTER #2\n",
      "['epidemiologo', 'settimana', 'possibile', 'caso', 'virologa', 'nuovo', 'parola', 'studiare', 'rischiare', 'notizia', 'burioni', 'rispondere', 'scientifico', 'università', 'dato', 'fase', 'italiano', 'spiegare', 'pandemia', 'contagio', 'epidemia', 'contagiare', 'emergenza', 'parlare', 'italia', 'virus', 'covid', 'virologo', 'esperto', 'coronavirus']\n",
      "\n",
      "\n",
      "TOP 30 PAROLE DEL CLUSTER #3\n",
      "['restare', 'ospite', 'pubblicare', 'aspettare', 'stare', 'bisognare', 'vero', 'pensare', 'pandemia', 'scoprire', 'comune', 'portare', 'esperto', 'video', 'salute', 'mascherina', 'fisico', 'spiegare', 'italiano', 'facebook', 'giulio', 'tarro', 'covid', 'virologo', 'parlare', 'dirigere', 'post', 'emergenza', 'medicare', 'coronavirus']\n",
      "\n",
      "\n",
      "TOP 30 PAROLE DEL CLUSTER #4\n",
      "['basire', 'territorio', 'domiciliare', 'famiglia', 'informazione', 'regionale', 'distanza', 'seguire', 'contagiare', 'possibile', 'servizio', 'evitare', 'comune', 'personale', 'sintomo', 'necessario', 'medico', 'marzo', 'disposizione', 'cittadino', 'misura', 'contattare', 'attività', 'regione', 'salute', 'sanitario', 'emergenza', 'covid', 'coronavirus', 'medicare']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Costruzione dizionario per il topic\n",
    "\n",
    "dtm = cv.fit_transform(corpus)\n",
    "LDA = LatentDirichletAllocation(n_components=5,n_jobs=-1) # funzione per specificare il numero di cluster\n",
    "LDA.fit(dtm)\n",
    "\n",
    "\n",
    "#len(cv.get_feature_names())\n",
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f'TOP 30 PAROLE DEL CLUSTER #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-30:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10088)\t1\n",
      "  (0, 42839)\t1\n",
      "  (0, 57905)\t1\n",
      "  (0, 26250)\t2\n",
      "  (0, 55122)\t1\n",
      "  (0, 76654)\t2\n",
      "  (0, 62152)\t2\n",
      "  (0, 68055)\t1\n",
      "  (0, 71972)\t1\n",
      "  (0, 41919)\t4\n",
      "  (0, 25117)\t5\n",
      "  (0, 1043)\t5\n",
      "  (0, 80571)\t1\n",
      "  (0, 68197)\t1\n",
      "  (0, 44096)\t2\n",
      "  (0, 9910)\t1\n",
      "  (0, 56725)\t1\n",
      "  (0, 23831)\t1\n",
      "  (0, 31127)\t1\n",
      "  (0, 33789)\t1\n",
      "  (0, 19216)\t3\n",
      "  (0, 80245)\t1\n",
      "  (0, 40594)\t1\n",
      "  (0, 64655)\t1\n",
      "  (0, 70869)\t1\n",
      "  :\t:\n",
      "  (59394, 75232)\t1\n",
      "  (59394, 91690)\t1\n",
      "  (59394, 64374)\t1\n",
      "  (59394, 56809)\t1\n",
      "  (59394, 59312)\t2\n",
      "  (59394, 21834)\t1\n",
      "  (59394, 55113)\t1\n",
      "  (59394, 79619)\t1\n",
      "  (59394, 42443)\t1\n",
      "  (59394, 80322)\t2\n",
      "  (59394, 89293)\t1\n",
      "  (59394, 67760)\t1\n",
      "  (59394, 50705)\t1\n",
      "  (59394, 30755)\t1\n",
      "  (59394, 8369)\t1\n",
      "  (59394, 60641)\t2\n",
      "  (59394, 42585)\t1\n",
      "  (59394, 65503)\t1\n",
      "  (59394, 90707)\t1\n",
      "  (59394, 62352)\t1\n",
      "  (59394, 50412)\t1\n",
      "  (59394, 63538)\t1\n",
      "  (59394, 61340)\t1\n",
      "  (59394, 77424)\t1\n",
      "  (59394, 20182)\t1\n"
     ]
    }
   ],
   "source": [
    "# Creazione grafico\n",
    "\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "number_topics=5\n",
    "count_data = dtm\n",
    "\n",
    "A=len(cv.get_feature_names())\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared2_3_'+str(number_topics))\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute visualization prep yourself\n",
    "\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = sklearn_lda.prepare(LDA, dtm, cv, mds='mmds')\n",
    "    #print (LDAvis_prepared)\n",
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared,f,0)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared2_3_'+ str(number_topics) +'.html')\n",
    "\n",
    "count_data = cv.fit_transform(df['Tot'])\n",
    "print(count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show graph\n",
    "x = range(start, limit, step)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "# plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
